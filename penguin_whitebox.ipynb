{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "random.seed(113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv(r'C:\\braindedmemory\\CP\\INPYTH\\pengin nn\\allisonhorst-palmerpenguins-e5bfd5f\\inst\\extdata\\penguins.csv')\n",
    "penguins =penguins.drop(columns=['island','sex','year']).dropna()\n",
    "penguins.to_csv('penguin-clean-train.csv',index=False)\n",
    "# penguins_features=penguins_filtered.drop(columns=['species'])\n",
    "# target=pd.get_dummies(penguins_filtered['species'])\n",
    "# target.head()\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# x_train,x_test,y_train,y_test= train_test_split(penguins_features,target,test_size=0.2,random_state=0,shuffle=True,stratify=target)\n",
    "with open(r'C:\\braindedmemory\\CP\\INPYTH\\pengin nn\\penguin-clean-train.csv') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader, None) # skip header\n",
    "    datatrain = list(csvreader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in datatrain:\n",
    "    if row[0]==\"Adelie\":\n",
    "        row[0] = 0\n",
    "    elif row[0]==\"Gentoo\":\n",
    "        row[0] = 1\n",
    "    else:\n",
    "        row[0] = 2\n",
    "    row[1:] = map(float, row[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# Split x and y (feature and target)\n",
    "train_X = [data[1:] for data in datatrain]\n",
    "train_y = [data[0] for data in datatrain]\n",
    "\n",
    "# Min-max Scaling\n",
    "# palmer-penguin dataset has varying scales\n",
    "feat_len = len(train_X[0])\n",
    "data_len = len(train_X)\n",
    "mnm = [0]*feat_len\n",
    "mxm = [0]*feat_len\n",
    "print(feat_len)\n",
    "for f in range(feat_len):\n",
    "    mnm[f] = train_X[0][f]\n",
    "    mxm[f] = train_X[0][f]\n",
    "    for d in range(data_len):\n",
    "        mnm[f] = min(mnm[f], train_X[d][f])\n",
    "        mxm[f] = max(mxm[f], train_X[d][f])\n",
    "\n",
    "    for d in range(data_len):\n",
    "        train_X[d][f] = (train_X[d][f] - mnm[f]) / (mxm[f] - mnm[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 50 - Loss: 0.056482408302348866\n",
      "1000 / 50 - Loss: 0.003740195328853507\n",
      "2000 / 50 - Loss: 0.003130173002849411\n",
      "3000 / 50 - Loss: 0.0027810933028934743\n",
      "4000 / 50 - Loss: 0.0025214841914551495\n",
      "5000 / 50 - Loss: 0.002360083044915221\n",
      "6000 / 50 - Loss: 0.0022484615620475385\n",
      "7000 / 50 - Loss: 0.002149181929571576\n"
     ]
    }
   ],
   "source": [
    "def matrix_mul_bias(A, B, bias): # Matrix multiplication (for Testing)\n",
    "    C = [[0 for i in range(len(B[0]))] for i in range(len(A))]    \n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(B[0])):\n",
    "            for k in range(len(B)):\n",
    "                C[i][j] += A[i][k] * B[k][j]\n",
    "            C[i][j] += bias[j]\n",
    "    return C\n",
    "\n",
    "def vec_mat_bias(A, B, bias): # Vector (A) x matrix (B) multiplication\n",
    "    C = [0 for i in range(len(B[0]))]\n",
    "    for j in range(len(B[0])):\n",
    "        for k in range(len(B)):\n",
    "            C[j] += A[k] * B[k][j]\n",
    "            C[j] += bias[j]\n",
    "    return C\n",
    "\n",
    "\n",
    "def mat_vec(A, B): # Matrix (A) x vector (B) multipilicatoin (for backprop)\n",
    "    C = [0 for i in range(len(A))]\n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(B)):\n",
    "            C[i] += A[i][j] * B[j]\n",
    "    return C\n",
    "\n",
    "def sigmoid(A, deriv=False):\n",
    "    if deriv: # derivation of sigmoid (for backprop)\n",
    "        for i in range(len(A)):\n",
    "            A[i] = A[i] * (1 - A[i])\n",
    "    else:\n",
    "        for i in range(len(A)):\n",
    "            A[i] = 1 / (1 + math.exp(-A[i]))\n",
    "    return A\n",
    "\n",
    "# Define parameter\n",
    "learening = 0.05\n",
    "epoch = 8000\n",
    "layers = [4, 10, 3] # number of neuron each layer\n",
    "\n",
    "# Initiate weight and bias with 0 value\n",
    "weight = [[0 for j in range(layers[1])] for i in range(layers[0])]\n",
    "weight_2 = [[0 for j in range(layers[2])] for i in range(layers[1])]\n",
    "bias = [0 for i in range(layers[1])]\n",
    "bias_2 = [0 for i in range(layers[2])]\n",
    "\n",
    "# Initiate weight with random between -1.0 ... 1.0\n",
    "for i in range(layers[0]):\n",
    "    for j in range(layers[1]):\n",
    "        weight[i][j] = 2 * random.random() - 1\n",
    "\n",
    "for i in range(layers[1]):\n",
    "    for j in range(layers[2]):\n",
    "        weight_2[i][j] = 2 * random.random() - 1\n",
    "\n",
    "prev_cost=0\n",
    "for e in range(epoch):\n",
    "    cost_total = 0\n",
    "    for idx, x in enumerate(train_X): # Update for each data; SGD\n",
    "        \n",
    "        # Forward propagation\n",
    "        h_1 = vec_mat_bias(x, weight, bias)\n",
    "        X_1 = sigmoid(h_1)\n",
    "        h_2 = vec_mat_bias(X_1, weight_2, bias_2)\n",
    "        X_2 = sigmoid(h_2)\n",
    "        \n",
    "        # Convert to One-hot target\n",
    "        target = [0, 0, 0]\n",
    "        target[int(train_y[idx])] = 1\n",
    "\n",
    "        # Cost function, Square Root Eror\n",
    "        eror = 0\n",
    "        for i in range(layers[2]):\n",
    "            eror +=  (target[i] - X_2[i]) ** 2 \n",
    "        cost_total += eror * 1 / layers[2]\n",
    "\n",
    "        # Backward propagation\n",
    "        # Update weight_2 and bias_2 (layer 2)\n",
    "        delta_2 = []\n",
    "        for j in range(layers[2]):\n",
    "            delta_2.append(-1 * 2. / layers[2] * (target[j]-X_2[j]) * X_2[j] * (1-X_2[j]))\n",
    "\n",
    "        for i in range(layers[1]):\n",
    "            for j in range(layers[2]):\n",
    "                weight_2[i][j] -= learening * (delta_2[j] * X_1[i])\n",
    "                bias_2[j] -= learening * delta_2[j]\n",
    "        \n",
    "        # Update weight and bias (layer 1)\n",
    "        delta_1 = mat_vec(weight_2, delta_2)\n",
    "        for j in range(layers[1]):\n",
    "            delta_1[j] = delta_1[j] * (X_1[j] * (1-X_1[j]))\n",
    "        \n",
    "        for i in range(layers[0]):\n",
    "            for j in range(layers[1]):\n",
    "                weight[i][j] -=  learening * (delta_1[j] * x[i])\n",
    "                bias[j] -= learening * delta_1[j]\n",
    "    # print(prev_cost)\n",
    "    if(cost_total<0.001):\n",
    "        break\n",
    "    # else:\n",
    "    #     prev_cost=cost_total\n",
    "\n",
    "    cost_total /= len(train_X)\n",
    "    if(e % 1000 == 0):\n",
    "        print(e,\"- Loss:\",cost_total) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "test.to_csv('penguin-clean-test.csv',index=False)\n",
    "with open(r'C:\\Users\\cmath\\Downloads\\Datasets\\Datasets\\penguins-clean-test.csv') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    next(csvreader, None) # skip header\n",
    "    datatest = list(csvreader)\n",
    "\n",
    "# Change string value to numeric\n",
    "for row in datatest:\n",
    "    if row[0]==\"Adelie\":\n",
    "        row[0] = 0\n",
    "    elif row[0]==\"Gentoo\":\n",
    "        row[0] = 1\n",
    "    else:\n",
    "        row[0] = 2\n",
    "\n",
    "    row[1:] = map(float, row[1:])\n",
    "\n",
    "# Split x and y (feature and target)\n",
    "test_X = [data[1:] for data in datatest]\n",
    "test_y = [data[0] for data in datatest]\n",
    "\n",
    "# Min-max Scaling\n",
    "feat_len = len(test_X[0])\n",
    "data_len = len(test_X)\n",
    "for f in range(feat_len):\n",
    "    for d in range(data_len):\n",
    "        test_X[d][f] = (test_X[d][f] - mnm[f]) / (mxm[f] - mnm[f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 1, 2, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 2, 1, 1, 2, 2, 0, 0, 1, 0, 0, 2, 0, 2, 1, 2, 2, 2, 0, 2, 0, 0, 1, 2, 2, 2, 1, 1, 1, 0, 1, 1, 1, 2, 2, 0, 1, 0, 1, 2, 0, 0, 0, 2, 2, 0, 2, 0, 1, 1, 2, 2, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 2, 1, 0, 0, 2, 1, 2, 1, 2, 0, 2, 2, 2, 0, 1, 1, 1]\n",
      "90.19607843137256 %\n"
     ]
    }
   ],
   "source": [
    "res = matrix_mul_bias(test_X, weight, bias)\n",
    "res_2 = matrix_mul_bias(res, weight_2, bias)\n",
    "\n",
    "preds = []\n",
    "for r in res_2:\n",
    "    preds.append(max(enumerate(r), key=lambda x:x[1])[0])\n",
    "\n",
    "# Print prediction\n",
    "print(preds)\n",
    "\n",
    "# Calculate accuration\n",
    "acc = 0.0\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == int(test_y[i]):\n",
    "        acc += 1\n",
    "print(acc / len(preds) * 100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b124afded074c70b1168b021b33622efccdf1097854b00f22ff1bd7656ffa95"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
